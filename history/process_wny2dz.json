[{
  "history_id" : "JgKAzkETUJbG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf_Jun14.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317724\n",
  "history_begin_time" : 1655309069523,
  "history_end_time" : 1656319034768,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "QpVpvZwzKKd8",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317275\n",
  "history_begin_time" : 1655240792433,
  "history_end_time" : 1655308889463,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YiUac61u0NQs",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 317274\n",
  "history_begin_time" : 1655240750532,
  "history_end_time" : 1655308888978,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "06xaefcrFtgo",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309474\n",
  "history_begin_time" : 1655211911278,
  "history_end_time" : 1655214661551,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "iLhFC3pj2OCm",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309359\n",
  "history_begin_time" : 1655176898978,
  "history_end_time" : 1655214662184,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "vo4duplBk62q",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309343\n",
  "history_begin_time" : 1655174481120,
  "history_end_time" : 1655214662658,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "W0OalQdYgJKf",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309342\n",
  "history_begin_time" : 1655174426600,
  "history_end_time" : 1655214663796,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "HkNaF85gIEwR",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3', 'CMAQ12KM_O3(ppb)', 'Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3', 'CMAQ12KM_O3(ppb)']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 309206\n",
  "history_begin_time" : 1655161058930,
  "history_end_time" : 1655214664277,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "l96UTcOSv9BW",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 294124\n",
  "history_begin_time" : 1655075899025,
  "history_end_time" : 1655214664758,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "baa04m11lwy",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 293687\n",
  "history_begin_time" : 1655073603587,
  "history_end_time" : 1655073626601,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "yDvdEvgMmIn4",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 269679\n",
  "history_begin_time" : 1654612002594,
  "history_end_time" : 1654619308939,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "hwrti1aWxHMv",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsleep 20",
  "history_output" : "Submitted batch job 269677\n",
  "history_begin_time" : 1654611898350,
  "history_end_time" : 1654619308373,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "Lep1ufXcecjC",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 269675\n",
  "history_begin_time" : 1654611695981,
  "history_end_time" : 1654619307839,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "3xBA7r0B2HkM",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268230\n",
  "history_begin_time" : 1654536599783,
  "history_end_time" : 1654619307243,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "Z5kUr2I8qzhb",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nprint('IT WORKED')\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654536360346,
  "history_end_time" : 1654536532829,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "wn2hbzhu2ay",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\n#rf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\n#pickle.dump(rf, open(filename, 'wb'))\nprint(\"IT WORKED\")\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654535458975,
  "history_end_time" : 1654535464146,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : "p6wvf2",
  "indicator" : "Done"
},{
  "history_id" : "MsjbjCC2EwMj",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /groups/ESS/aalnaim/cmaq/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268141\n",
  "history_begin_time" : 1654491910593,
  "history_end_time" : 1654535259974,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "cvbKVxUSdrbt",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/groups/ESS/aalnaim/cmaq/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm\n\nsbatch /groups/ESS/aalnaim/cmaq/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268139\n",
  "history_begin_time" : 1654491776428,
  "history_end_time" : 1654535260558,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tGQb2syaWuR0",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268137\n",
  "history_begin_time" : 1654491629015,
  "history_end_time" : 1654535261059,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "usex3SwJ6mTS",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268135\n",
  "history_begin_time" : 1654491425039,
  "history_end_time" : 1654535261590,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "xt19rQLKb5hk",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\necho \"# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\" >> /home/aalnaim/rf_pyCaret.py\n\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\necho # Write first python in Geoweaver# NASA GEOWEAVER\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-xt19rQLKb5hk.sh: line 25: from: command not found\n./geoweaver-xt19rQLKb5hk.sh: line 26: from: command not found\nimport: unable to open X server `' @ error/import.c/ImportImageCommand/344.\n./geoweaver-xt19rQLKb5hk.sh: line 28: from: command not found\n./geoweaver-xt19rQLKb5hk.sh: line 31: syntax error near unexpected token `('\n./geoweaver-xt19rQLKb5hk.sh: line 31: `home = str(Path.home())'\n",
  "history_begin_time" : 1654491367280,
  "history_end_time" : 1654491394414,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "q6ogNuO8i4ck",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\n\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268133\n",
  "history_begin_time" : 1654491162242,
  "history_end_time" : 1654491393867,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "nfkZqgovRjlG",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>/home/aalnaim/rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython /home/aalnaim/rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268131\n",
  "history_begin_time" : 1654490903840,
  "history_end_time" : 1654491393398,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "PamN9MxUU1K4",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> /home/aalnaim/cmaq_gpu.slurm\n\nsbatch /home/aalnaim/cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268130\n",
  "history_begin_time" : 1654490758893,
  "history_end_time" : 1654491392901,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "zTC26oVAvMNv",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\" >> cmaq_gpu.slurm\n\n",
  "history_output" : "Running",
  "history_begin_time" : 1654490278351,
  "history_end_time" : 1654490735341,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "VfWXgYKIY46f",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268129\n",
  "history_begin_time" : 1654490091612,
  "history_end_time" : 1654490734793,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "OsW06ht15wmz",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" >> cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268128\n",
  "history_begin_time" : 1654490008108,
  "history_end_time" : 1654490081925,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "yRMgjX7O036v",
  "history_input" : "echo \"#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\" > cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "Submitted batch job 268127\n",
  "history_begin_time" : 1654489311859,
  "history_end_time" : 1654489851171,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "dUmujbU0K8pG",
  "history_input" : "echo \"\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\n\" > cmaq_gpu.slurm\n\nsbatch cmaq_gpu.slurm",
  "history_output" : "sbatch: error: This does not look like a batch script.  The first\nsbatch: error: line must start with #! followed by the path to an interpreter.\nsbatch: error: For instance: #!/bin/sh\n",
  "history_begin_time" : 1654489261923,
  "history_end_time" : 1654489851834,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "9VEMKfOKzdSL",
  "history_input" : "cat <<EOF >>cmaq_gpu.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\nEOF\n\necho \"Done!\"\necho $(pwd)",
  "history_output" : "./geoweaver-9VEMKfOKzdSL.sh: line 59: EOF: command not found\nDone!\n/home/aalnaim/gw-workspace/9VEMKfOKzdSL\n",
  "history_begin_time" : 1654488845481,
  "history_end_time" : 1654489852569,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "YZpAntSz921L",
  "history_input" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=python-gpu\n#SBATCH --output=python-gpu.%j.out\n#SBATCH --error=python-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv(home+'/cmaq/training.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = home+'/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py",
  "history_output" : "./geoweaver-YZpAntSz921L.sh: line 16: nvidia-smi: command not found\nLmod has detected the following error: The following module(s) are unknown:\n\"hosts/dgx\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date; it may help to try:\n  $ module --ignore-cache load \"hosts/dgx\"\n\nAlso make sure that all modulefiles written in TCL start with the string\n#%Module\n\n\n\n   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   29.489082   -81.276833        1.0  ...      3   15     12\n1   40.580200   -74.199402       37.0  ...      3   15     12\n2   39.128860   -84.504044       36.0  ...      3   15     12\n3   41.096157   -80.658905       26.0  ...      3   15     12\n4   34.635960   -82.810669       39.0  ...      3   15     12\n\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1654488620622,
  "history_end_time" : 1654488708322,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "XCUgtFWm1NCS",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-XCUgtFWm1NCS.sh: line 61: python: command not found\n./geoweaver-XCUgtFWm1NCS.sh: line 62: EOF: command not found\nSubmitted batch job 268125\n",
  "history_begin_time" : 1654488451251,
  "history_end_time" : 1654488696580,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "eIfu6yALskJK",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\npython <<EOF \n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\n\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-eIfu6yALskJK.sh: line 62: EOF: command not found\nSubmitted batch job 268124\n",
  "history_begin_time" : 1654488395089,
  "history_end_time" : 1654488418021,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "vhhKiNPD7vzO",
  "history_input" : "cat <<EOF >>cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF >>rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-vhhKiNPD7vzO.sh: line 61: python: command not found\n./geoweaver-vhhKiNPD7vzO.sh: line 62: EOF: command not found\nSubmitted batch job 268123\n",
  "history_begin_time" : 1654488249524,
  "history_end_time" : 1654488417507,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "AuddkGqBHLQo",
  "history_input" : "cat <<EOF > cmaq.slurm\n#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-AuddkGqBHLQo.sh: line 61: python: command not found\n./geoweaver-AuddkGqBHLQo.sh: line 62: EOF: command not found\nSubmitted batch job 268122\n",
  "history_begin_time" : 1654487557488,
  "history_end_time" : 1654488416989,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "S4lxbt3v3lbO",
  "history_input" : "#!/bin/bash\ncat <<EOF > cmaq.slurm\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py\nEOF\nsbatch cmaq.slurm",
  "history_output" : "./geoweaver-S4lxbt3v3lbO.sh: line 61: python: command not found\n./geoweaver-S4lxbt3v3lbO.sh: line 62: EOF: command not found\nsbatch: error: This does not look like a batch script.  The first\nsbatch: error: line must start with #! followed by the path to an interpreter.\nsbatch: error: For instance: #!/bin/sh\n",
  "history_begin_time" : 1654487491532,
  "history_end_time" : 1654488416191,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "myxZd3pn5O6C",
  "history_input" : "#!/bin/bash\n#SBATCH --partition=gpuq                    # the DGX only belongs in the 'gpu'  partition\n#SBATCH --qos=gpu                           # need to select 'gpu' QoS\n#SBATCH --job-name=cmaq-gpu\n#SBATCH --output=cmaq-gpu.%j.out\n#SBATCH --error=cmaq-gpu.%j.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=64                 # up to 128;\n#SBATCH --gres=gpu:A100.40gb:4              # up to 8; only request what you need\n#SBATCH --mem-per-cpu=3500M                 # memory per CORE; total memory is 1 TB (1,000,000 MB)\n#SBATCH --export=ALL\n#SBATCH --time=0-04:00:00                   # set to 1hr; please choose carefully\nset echo\numask 0027\n# to see ID and state of GPUs assigned\nnvidia-smi\nmodule load hosts/dgx                        # switch to the modules on the dgx\nmodule load python\nsource /home/aalnaim/CMAQAI/bin/activate\n\ncat <<EOF > rf_pyCaret.py\n# Write first python in Geoweaver# NASA GEOWEAVER\n# CMAQ-AI Model: Training Voting-XGBoost model\n\n# Importing necessary libraries\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport pickle\nfrom pathlib import Path\n\n# home directory\nhome = str(Path.home())\n\n# importing data\nfinal=pd.read_csv('/groups/ESS/mislam25/processed_training/agg_data_2021_03_15_to_22_4_30.csv')\nprint(final.head())\nfinal=final.dropna()\n\n# Processing training  data\nX = final.drop(['AirNOW_O3','Latitude_x','Longitude_x'],axis=1)\ny = final['AirNOW_O3']\n\nrf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=-1, oob_score=False,\n                      random_state=3086, verbose=0, warm_start=False)\n\nrf.fit(X, y)\n\n# save the model to disk\nfilename = '/groups/ESS/aalnaim/cmaq/models/rf.sav'\n#filename = 'D:/Research/CMAQ/local_test/xgboost.sav'\npickle.dump(rf, open(filename, 'wb'))\nEOF\npython rf_pyCaret.py",
  "history_output" : "./geoweaver-myxZd3pn5O6C.sh: line 16: nvidia-smi: command not found\nLmod has detected the following error: The following module(s) are unknown:\n\"hosts/dgx\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date; it may help to try:\n  $ module --ignore-cache load \"hosts/dgx\"\n\nAlso make sure that all modulefiles written in TCL start with the string\n#%Module\n\n\n\n   Latitude_x  Longitude_x  AirNOW_O3  ...  month  day  hours\n0   29.489082   -81.276833        1.0  ...      3   15     12\n1   40.580200   -74.199402       37.0  ...      3   15     12\n2   39.128860   -84.504044       36.0  ...      3   15     12\n3   41.096157   -80.658905       26.0  ...      3   15     12\n4   34.635960   -82.810669       39.0  ...      3   15     12\n\n[5 rows x 18 columns]\n/home/aalnaim/CMAQAI/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n  warn(\n",
  "history_begin_time" : 1654487302837,
  "history_end_time" : 1654487384411,
  "history_notes" : null,
  "history_process" : "wny2dz",
  "host_id" : null,
  "indicator" : "Stopped"
},]
